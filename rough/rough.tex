\documentclass[12pt,a4paper]{report}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\author{Lauren Frazier}
\title{Evaluation of Gesture-Based Controls for Robotic Systems}
\begin{document}
\maketitle
%%%%%%%%%%%%%%%%%%%%%%% TODO LIST %%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\todo}[1]{
    \addcontentsline{tdo}{todo}{\protect{#1}}
    \marginpar{#1}
}

\makeatletter \newcommand \listoftodos{\section*{Todo list} \@starttoc{tdo}}
  \newcommand\l@todo[2]
    {\par\noindent \textit{#2}, \parbox{10cm}{#1}\par} \makeatother
    
    
%\listoftodos
%%%%%%%%%%%%%%%%%%%%% END TODO LIST %%%%%%%%%%%%%%%%%%%%%%%%%

\setcounter{page}{1}
\pagenumbering{roman}
\tableofcontents
\listoftables
\listoffigures

\chapter*{Acknowledgements}
\addcontentsline{toc}{chapter}{Acknowledgements}
These are acknowledgements.

\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}
This thesis aims to test the effectiveness/ease of use of smartphone gesture-based robotic control systems vs. traditional control systems.  Robotic control systems are becoming more common, especially in the military. Arm and hand gestures are typical human forms of communication, so applying that to a robotic control system can yield a more intuitive system. With military applications, there are lives at stake, so having the most efficient, intuitive control system can make a large difference in the success of a mission and the safety of the soldiers involved. 
"Interactions and Training with Unmanned Systems and the Nintendo Wiimote" (Varcholik, Barber, and Nicholson) describes a gesture based control system that uses the Nintendo Wiimote to determine arm/hand gestures and control a robot. I propose to create a gesture based system using a smartphone and conduct an experiment similar to Varcholik, Barber, and Nicholson, collecting survey data from the participants on the the effectiveness and ease of use of each system.

\chapter{Introduction}
\pagestyle{headings}
\setcounter{page}{1}
\pagenumbering{arabic}

This thesis aims to test both the perceived and actual effectiveness and ease of use of smartphone gesture-based robotic control systems vs. traditional control systems.

The need for human-robot interfaces is increasing rapidly. The military has already begun using unmanned vehicles in several different arenas (air, ground, water). In order to develop the most efficient human-robot interface, we turned to a traditional form of human-human communication, arm and hand gestures. Arm and hand gestures are typical human forms of communication, so applying that to a robotic control system can yield a more intuitive system. With military applications, there are lives at stake, so having the most efficient, intuitive control system can make a large difference in critical moments and improve the safety of those involved. A more intuitive system will also reduce the training time and expenses for the operators of the vehicle.

``Interactions and Training with Unmanned Systems and the Nintendo Wiimote" \cite{Varcholik_Barber_Nicholson_2008} describes a gesture based control system that uses the Nintendo Wiimote to determine arm/hand gestures and control a robot. They then conducted a study where subjects used Wiimote gesture system and a more standard system and filled out a survey to indicate how effective the Wiimote system was as compared to the standard control system for the robot.

I proposed a gesture based system using a smartphone and conducted a human factors experiment similar to Varcholik, Barber, and Nicholson. The system uses a Samsung Galaxy S II phone for the gesture-based input, a tilt-based controller, and a touch screen D-Pad controller, and a Microsoft XBOX 360 controller for the more traditional input. Subjects used each of the four controls in a random order to guide an iRobot Roomba vacuum cleaner through a short time trial course. The raw data (video and observations during the time trials) was analyzed with metrics like the time required to complete the course, number of times the subject went outside the course boundaries, number of times the subject acknowledged making a mistake, etc. After the experiment is complete, subjects also filled out a survey, and both sets of data were used to determine which control scheme is more effective and intuitive. 

% Contributions
%This paper makes the following contributions:
%(1)  Introduces a gesture based control system using a smartphone.
%(2)  Provides data showing physical reactions to different control systems in addition to survey results after the experiment.
This thesis introduces an inexpensive gesture based control system that uses commercial, off the shelf hardware (an Android smartphone and a Roomba vacuum cleaner). It also provides data showing both subjects' objective performance and their perceived experience with each controller.

% Document articulation
The rest of this thesis is organized as follows. In the next chapter, I present a taxonomy of related work. Chapter 3 provides implementation details for the different controls. Chapter 4 describes the design of the human factors experiment, including choice of hardware, track design, experimental procedure, and the questionnaire. In Chapter 5, I analyze and discuss the results of the experiment. Chapter 6 provides the conclusion and a brief summary. 

\chapter{Related Work}
While this paper has most of its origin in \cite{Varcholik_Barber_Nicholson_2008}, there are a few differences. \cite{Varcholik_Barber_Nicholson_2008} uses a Nintendo Wiimote to control the actions of the robot, rather than a smartphone or XBOX controller. In the case of the smartphone, computation of gestures is performed on the phone, rather than on an intermediate computer.

There is a large body of work about robotic control systems that led up to this point.  \cite{Varcholik_Barber_Nicholson_2008} references three papers in human-robot interaction: \cite{Waldherr}, \cite{Rogalla}, and \cite{Guo}. \cite{Waldherr} presents a computer vision based approach to human-robot communication where the computation is done onboard the robot. \cite{Rogalla} presents work on using both gestures and speech. The speech commands are used to augment and clarify the hand gesture commands. \cite{Guo} presents work on using the Wiimote to control a robotic dog, even showing that the Wiimote
outperformed the keyboard.

\cite{Waldherr} takes a computer vision based approach to gesture recognition. It explores several methods like template-based matching and neural networks. It is derived from several other vision-based works, like Kortenkamp, which used optical flow to recognize up to 6 gestures, and Kahn, which recognizes pointing gestures using feature maps and the color of the user's clothing.

\cite{Rogalla} integrates speech recognition into the system, but it also uses a computer vision based gesture recognizer. It draws from \cite{Kestler}, which recognizes static hand gestures from still photos, and CORA, a robotic assistant that uses speech and deictic gestures to execute commands (ex. ``Turn and face me").

\cite{Guo} uses a Wiimote to control a Sony AIBO robotic dog. It describes a guide to designing effective human-robot interface \cite{GoodrichOlsen}. It also describes \cite{YancoDrury}, which classifies and details robotic control schemes and defines terminology related to HRI. \cite{YancoDrury} defines the autonomy level of a robot as the percentage of the time that the robot carries out a task on its own (as opposed to intervention from the operator). Like \cite{Guo}, this paper deals with a robot with a low autonomy level.

\chapter{Implementation of Control Systems}
In this chapter, I will describe the implementation of the four control systems used in the experiment. First, I describe the ``standard" XBOX 360 controller setup, then the application used for the D-Pad and tilt controllers, and finally the gesture-based control application.

The robot in question is an iRobot Roomba 560 vacuum  cleaner. None of the four control schemes require making modifications to the Roomba, and the vacuum/brushes are turned off during the experiment. It is controlled from a standard wireless Microsoft XBOX 360 controller and a Samsung Galaxy S II Android phone.
\todo{photo of roomba and phone}
\section{XBOX 360 Controller}
The XBOX 360 controls for the Roomba are written in Python (see Appendix). The program uses a library called RoombaSCI to communicate with the Roomba via Bluetooth. RoombaSCI is a Python wrapper for iRobot's Serial Command Interface Specification, which allows commands to be sent to the Roomba through its serial port. RoombaSCI abstracts tasks like setting up the connection and sending bytes to specific motors into commands like ``forward", ``stop", and ``spin left" \cite{RoombaSCI}. It also allows the programmer to set the speed of the Roomba within the range allowed by the hardware (-500 - 500 mm/s). \cite{iRobot}

A RooTooth bluetooth-serial adapter was used to communicate with the computer wirelessly. The Pygame library is also used to properly capture the input from the controller. 

The end result is a controller that can be used to move forward/backward and rotate to the left or right in place. The right trigger sends the ``forward" command, the left trigger sends ``backward", and the left analog stick pressed to the left or right sends a ``left" or ``right" command. The robot continues moving forward/backward/left/right as long as the trigger or analog stick is held, and will stop when it is released, or another button is pressed. When using the XBOX 360 controller, the robot can only move at the maximum speed, and it can only perform one of forward/backward or rotation at any given time (forward motion must stop in order to turn).
\todo{xbox diagram}

\section{Cellbots Application}
The D-Pad and the tilt controls were downloaded to the phone as part of a single app called Cellbots. Cellbots is an open source Android application, available for free online or in the Android Market. The app comes with four control schemes, including the D-Pad controls and the tilt controls. It also included voice controls and an on-screen Atari-style joystick which were not used in the experiments but could be utilized in future research.
\todo{screenshots of cellbots}

The D-Pad controls require the user to press one of four on-screen buttons, arranged in a cross. Like the XBOX 360 controller, the robot moves forward/backward and rotates in place when the appropriate button is held down, stopping when the button is released. Also like the XBOX 360 controller, the D-Pad only allows one speed and one type of movement at a time.

The tilt controls use the phone's accelerometers to control the robot. The user holds the phone horizontally (screen facing up) and holds down an on-screen button. While the button is held down, tilting the phone forward (rotation around its x-axis) causes the robot to begin to drive forward. The phone can be rolled right or left (rotating around its y-axis) to steer. 
\todo{phone axis diagram}
The tilt controls differ from the other controls in two ways: first, the speed can vary. The more the phone is tilted forward, the faster the robot moves, up to the maximum speed of 500 mm/s.  The second difference is that the tilt controls allow forward/backward motion and turning at the same time. For example, if the phone is tilted forward and rolled left, the robot will perform a left turn, rather than rotating in place.

\section{Gesture-Based Control Application}
The gesture-based controls are implemented on top of the existing Cellbots application. Cellbots is an open source application, so the gesture controls were added as a fifth control type. \todo{screenshot of gesturetrainer}
To recognize the gestures using accelerometers only, I turned to another open source Android application called GestureTrainer. GestureTrainer is an implementation of the concepts outlined in \cite{TaKG} for gesture recognition using accelerometer data.

Users can record and name their own gestures on the phone, and from then on when the gesture is performed and recognized, the name is sent to the Roomba. The phone recognizes gestures continuously, without the need to specify when the user is about to perform a gesture.

To record a gesture, the user presses a button to start ``learning mode". The phone receives a constant stream of events for every movement that the accelerometer picks up in the form of a vector of floats, ($\Delta x, \Delta y, \Delta z$). If the norm of the vector is above a certain threshold, the movement is considered ``significant" and the app begins recording the stream of vectors. When the phone receives 10 events in a row whose norms do not exceed the threshold, the gesture is determined to be complete. The list of xyz-vectors is stored as a Gesture object, and given the name specified by the user. 

After exiting ``learning mode", the phone begins recognizing gestures immediately. When a gesture is detected, it is stored in a Gesture object, and sampled by one of several different feature extractors. The feature extractors serve to divide the gesture into a certain number of vectors, or normalize the values in the vectors. We then compute the distance between this sampled signal and each of the user's recorded gestures.

The distance calculation from Gesture $a$ to Gesture $b$ is done through the Dynamic Time Warping algorithm (see Appendix for source code). A $a.length \times b.length$ matrix of floats, $dist$, is constructed, where entry $dist(i,j)$ is the norm of the $i$th vector in $a$ minus the $j$th vector in $b$. A second matrix of the same size, $cost$, is constructed so that the first column of $cost$ is the same as the first column of $dist$, and the rest of the entries are 0.

The $cost$ matrix is filled in dynamically from the top left ($cost(0, 0)$) to the bottom right ($cost(a.length, b.length)$), iterating over columns first, then rows. To compute $cost(i, j)$, the current minimum cost is computed as \[minCost = cost(i - 1, j - 1) + dist(i, j)\] This is the value assigned to $cost(i, j)$. If $cost(i - 1, j) + dist(i, j) < minCost$ or $cost(i, j - 1) + dist(i, j) < minCost$, an additional offset penalty of 0.5 is added to $cost(i, j)$.

After the whole $cost$ matrix has been filled in, the entry $cost(a.length - 1, b.length - 1)$ (the bottom right corner), is the minimum distance between the two Gestures $a$ and $b$. The stored Gesture with the smallest distance from the performed Gesture is the one that is selected to be sent to the Roomba. The last step is to check that distance against a threshold. If the distance is less than the threshold, then the command would be sent, otherwise no command is sent. This is to ensure that all commands sent were close matches, since every gesture will return a match with some stored gesture, even if there are no similar gestures. If the closest match has a high distance, it is not close enough to be executed as a command so it is ignored.

This code was integrated into Cellbots as another control view, to ensure that the same speeds/timing would be used for each control scheme on the phone.
\todo{screenshot}

The gestures defined were ``forward", ``backward", ``left", and ``right". 
\todo{photos of gestures}
All the gestures are performed with the phone's screen facing the user. The ``stop" command was left as an on-screen button, rather than a gesture, to make it easier for the subject to maintain control (and for the sake of the hardware and testing environment).

\chapter{Human Factors Experiment Design}
General description and justification for choices in the experiment.
\section{Platform}
\section{Track Design}
\section{Questionnaire Design}
\section{Experiment Procedure}

\chapter{Results}
In this chapter, I will discuss the results of the experiments. 
\section{Time Trials}
\section{Questionnaire}
\section{Analysis and Hypothesis}

\chapter{Conclusion}
This is a conclusion.

\bibliographystyle{plain}
\bibliography{rough}

\end{document}